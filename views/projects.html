<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link rel="stylesheet" href="../styles/projects.css">
        <script src="../scripts/main.js"></script> 
        <link href="https://fonts.googleapis.com/css2?family=Lato:wght@400;700&display=swap" rel="stylesheet">

        <title>Projects</title>
    </head>

    <body onload = "load('projects')">
        <div id="colContainer">
            <div class="projectsContainer">
                <div class ="leftCol">
                    <div class="mainText">
                        <h2><a href="https://github.com/CalebEng/Emotion-Detection-CPS-843-Project-" target="_blank" rel="noopener">Emotion Detection Program</a></h2>
                        <h3>CPS-843 (Computer Vision) | Fall 2024</h3>
                        <button class="expandBut" data-project="emotion">More Info</button>
                    </div>
                    

                    <div class="mainText">
                        <h2><a href="https://github.com/CalebEng/Silverlight-bot" target="_blank" rel="noopener">Discord Bot</a></h2>
                        <h3>July 2023 - Present</h3>
                        <button class="expandBut" data-project="discord">More Info</button>
                    </div>

                    <div class="mainText">
                        <h2><a href="https://github.com/CalebEng/Zeppelin" target="_blank" rel="noopener">3D Graphics Project</a></h2>
                        <h3>Computer Graphics | Fall 2023</h3>
                        <button class="expandBut" data-project="zeppelin">More Info</button>
                    </div>
                </div>
            </div>
            <div class="projectsContainer">
                <div class = "rightCol hidden"> 
                    <div class="mainText">
                        <div class = "projectInfo hidden" data-project="emotion">
                            <h3>Overview</h3>
                            <hr>
                            <p>
                                Facial Emotion Recognition (FER) uses computational methods to examine and classify human emotions based on facial expressions. 
                                The project aims to develop an advanced FER application using machine learning techniques, specifically, Convolutional Neural Networks (CNNs), 
                                to identify seven distinct emotions: anger, disgust, fear, happy, sad, surprise, and neutral. 
                                The objective was to evaluate the performance of the CNN-based approach in recognizing emotions under various circumstances. 
                                The model was tested using local system environments as well as Google Colab, achieving an accuracy of 62.71%. Despite challenges 
                                such as dataset imbalance and computational limitations, these results demonstrate the potential of deep learning in emotion recognition tasks, 
                                though further tuning and refinement are needed to improve the modelâ€™s effectiveness.
                            </p>
                            
                            <h3>Dataset</h3>
                            <hr>
                            <p>
                                The <a href="https://www.kaggle.com/datasets/msambare/fer2013" target="_blank" rel="noopener">FER-2013</a> dataset was used to train our CNN. 
                                The dataset consists of two main folders, one for testing and one for training. 
                                The training dataset consists of 28709 examples used to train the model to identify emotions from the following seven facial expressions: 
                                anger, disgust, fear, happy, sad, surprise, and neutral. However, each emotion in the dataset has varying amounts of data. Anger contains 3589 photos, 
                                disgust has 436, fear has 4097, happiness has 7215, neutral has 4965, sad has 4830, and surprise has 3171. 
                                The test data contains 3589 examples that are used to test the model. Collectively, the selected dataset has 56.51 MB of data. 
                                Each data sample in the FER-2013 dataset is grayscale and the images' dimensionality is 48x48. 
                                This simplifies the data preprocessing step and ensures that our model can consistently learn and differentiate between emotions. 
                                Likewise, each data sample is variably unique due to the lighting, facial obstructions, and viewing angles, enabling the model to improve its generalizability.
                                Examples of each emotion:
                            </p>

                            <div id ="dataContainer">
                                <img id="dataset" src="../images/Data.png" alt = "">
                            </div>

                            <h3>Results</h3>
                            <hr>
                            <p>
                                At the end of the project 2 models were produced with performance around 60-65% accuracy.
                                Demo video made by a member of the project is available at: <a href="https://github.com/CalebEng/Emotion-Detection-CPS-843-Project-" target="_blank" rel="noopener">Emotion Detection Program</a>
                                <br>
                                <h4>Confusion Matrices:</h4>
                                <hr>
                                Local Machine:
                                <br>
                                <img id = "localMachine" src="../images/Local.png" alt="">
                                <br>
                                Google Colab:
                                <br>
                                <img id = "gc" src = "../images/GoogleColab.png" alt = "">
                            </p>
                            <p>
                                <br>
                                <h4>Classification Report:</h4>
                                <hr>
                                <img id = "Classification" src ="../images/classification.png" alt = "">
                            </p>
                            
                        </div>

                        <div class="projectInfo hidden" data-project="discord">
                            <h3>Overview</h3>
                            <hr>
                            <p>
                                The Silverlight discord bot, is an application made in Python that interacts with Discord's api.
                                Some of the functionalities include: games like TicTacToe and Hangman, prototype poem N-gram model made using TensorFlow,
                                and reading information/activities from users in the server.
                                <br>
                                <h4>Examples:</h4>
                                <hr>
                                Typing sl-info [name] will make the bot look for the user specified. If found on the server, the bot will read all the available 
                                information about the user and return it in a readable form.
                                <br>
                                <img src="../images/slinfo.png" alt="">
                                <br>
                                Typing sl-song [name] will make the bot look for the user specified. If no name is specified, 
                                the the bot will look at your information instead.
                                If found on the server, the bot will see if the user is listening to a song.
                                If they are, it will display information about the song and allow the user to listen along.
                                <br>
                                <img src="../images/slsong.png" alt="">
                                <br>
                                Using the hangman command will start a game of hangman using a list of 57,521 possible words.
                                <img src="../images/hangman.png" alt="">
                            </p>
                        </div>
                        <div class="projectInfo hidden" data-project="zeppelin">
                            <h3>Overview</h3>
                            <hr>
                            <p>
                                This C++ application showcases a 3D zeppelin simulation built using OpenGL and supporting libraries. 
                                This blimp is freely pilotable in all directions with a third person view by default. 
                                A computer controlled opponent roams the space and will open fire when you enter its attack radius.
                                Switching to first person view transforms the camera into a cockpit style perspective, 
                                complete with a targeting reticle that lets you aim and fire back at the enemy.
                            </p>

                            <h3>Implementation Details</h3>
                            <hr>
                            <p>
                                A demo video is available here: <a href="https://www.youtube.com/watch?v=5KTGa-xQKzM" target="_blank" rel="noopener">Demo test</a>
                            </p>
                            <p>
                                The blimps were made with simple OpenGL shapes, and are drawn using the following matrix math:
                            </p>
                            <p>
                                glPushMatrix() [saves the current transformation matrix]-> glTranslatef(x,y,z) [applies translation movement]-> glRoatef(angle,x,y,z) 
                                [applies rotation around a specified axis]-> glScalef(x,y,z) [applies scaling]-> 
                                specific shape is drawn and then modified be the prior function calls -> glPopMatrix() [restores the previous transformation matrix]
                            </p>

                            <p>
                                Camera movement:
                            </p>
                            <p>
                                The cameras position is calculated using spherical coordinates that are defined by a radius and two angles.
                                These are then converted into Cartesian coordinates. This helps the camera orbit smoothly around the blimp in third person and allows for smooth aim in first person.
                                <br><br>
                                The conversion follows this formula: <br><br>
                                    camZ = radius * cos(t_thi) * cos(t_theta);<br>
				                    camX = radius * cos(t_thi) * sin(t_theta);<br>
				                    camY = radius * sin(t_thi);<br>

                            <br><br>
                            These angles are then capped to -89 < t_thi * 180 / 3.1415 < 89 in third person and -89 < t_thi * 180 / 3.1415 <= 44 <br>
                            If the new proposed angle is out of bounds they are then stopped to prevent unnatural flipping. 
                            </p>
                        </div>
                    </div>
                    <div id = "test">
                        <button id = "closeBut" class = "closeBut">X</button>
                    </div>
                </div>
            </div>
        </div>
    </body>
</html>